## 术语
* GPT = Generative Pre-trained Transformer（生成式预训练Transfomer模型）
	* 基于转换器的生成式预训练模型（Generative pre-trained transformers; GPT）是OpenAI开发的一系列延伸自转换器架构（Transformer）的自然语言生成模型。）
* LLMs = Large Language Models (LLMs) 


While GPT-3 and GPT-4 are the most popular Large Language Models (LLMs) right now, over the next few years, there's likely to be a lot more competition. Google, for example, just unveiled Bard—its AI chatbot—which is powered by its own language engine called Language Model for Dialogue Applications (LaMDA). But for now, OpenAI's offering is the de facto industry standard. It's just the easiest tool for people to get their hands on.  


2.  无监督学习与有监督学习？
3.  深度神经网络、

# 2、GPT之技术演进时间线
GPT从开始至今，其发展历程如下：

2017年6月，Google发布论文《Attention is all you need》​，首次提出Transformer模型，成为GPT发展的基础。 论文地址： https://arxiv.org/abs/1706.03762

2018年6月,OpenAI 发布论文《Improving Language Understanding by Generative Pre-Training》(通过生成式预训练提升语言理解能力)​，首次提出GPT模型(Generative Pre-Training)。论文地址： https://paperswithcode.com/method/gpt 。

2019年2月，OpenAI 发布论文《Language Models are Unsupervised Multitask Learners》（语言模型应该是一个无监督多任务学习者），提出GPT-2模型。论文地址: https://paperswithcode.com/method/gpt-2

2020年5月，OpenAI 发布论文《Language Models are Few-Shot Learners》(语言模型应该是一个少量样本(few-shot)学习者，提出GPT-3模型。论文地址： https://paperswithcode.com/method/gpt-3

2022年2月底，OpenAI 发布论文《Training language models to follow instructions with human feedback》（使用人类反馈指令流来训练语言模型）​，公布Instruction GPT模型。论文地址： https://arxiv.org/abs/2203.02155

2022年11月30日，OpenAI推出ChatGPT模型，并提供试用，全网火爆。



## 相关材料
这篇《十分钟理解Transfomer》( https://zhuanlan.zhihu.com/p/82312421 ） 可以看一下。
